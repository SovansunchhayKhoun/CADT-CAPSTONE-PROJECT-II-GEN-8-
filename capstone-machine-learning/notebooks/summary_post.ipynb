{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-07-31T23:51:12.289824Z",
     "end_time": "2024-07-31T23:51:12.315525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " the internet of things (iot) refers to the billions of physical devices around the world that are now connected to the internet, all collecting and sharing data.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "The Internet of Things (IoT) refers to the billions of physical devices around the world that are now connected to the internet, all collecting and sharing data. Thanks to the arrival of super-cheap computer chips and the ubiquity of wireless networks, it's possible to turn anything, from something as small as a pill to something as big as an airplane, into a part of the IoT. Connecting all these different objects and adding sensors to them adds a level of digital intelligence to devices that would be otherwise dumb, enabling them to communicate real-time data without involving a human being. The Internet of Things is making the fabric of the world around us smarter and more responsive, merging the digital and physical universes.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)  # Remove references\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = sent_tokenize(processed_text)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(processed_text)\n",
    "filtered_words = [word for word in word_tokens if word not in stop_words and word.isalpha()]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = FreqDist(filtered_words)\n",
    "\n",
    "# Calculate sentence scores based on word frequencies\n",
    "sentence_scores = {}\n",
    "for sent in sentences:\n",
    "    for word in word_tokenize(sent):\n",
    "        if word in word_freq.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] = word_freq[word]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_freq[word]\n",
    "\n",
    "# Get the highest-scoring sentences\n",
    "num_sentences = int(len(sentences) * 0.3)  # Select 30% of sentences\n",
    "summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "# Join the summary sentences\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(\"Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
